{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distill a Reader\n",
    "\n",
    "- **Level**: Advanced\n",
    "- **Time to complete**: 30 minutes\n",
    "- **Prerequisites**: Prepare the Colab environment (see links below).\n",
    "- **Nodes Used**: `FARMReader`\n",
    "- **Goal**: Distil the question answering capabilities of a larger Reader model into a smaller Reader model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overview\n",
    "\n",
    "Model distillation is the process of teaching a smaller model to imitate the performance of a larger, better trained model. By distilling one model into another, you end up with a more computationally efficient version of the original with only a slight trade-off in accuracy. In this tutorial, you will learn how to perform one form of model distillation on Reader models in Haystack. Model distillation is a complex topic and an active area of research so if you would like to learn more about it, we recommend looking at [Model Distillation](https://docs.haystack.deepset.ai/docs/model_distillation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in GPU](https://docs.haystack.deepset.ai/docs/enable-gpu-runtime-in-colab)\n",
    "- [Check if GPU is Enabled](https://docs.haystack.deepset.ai/docs/check-if-gpu-is-enabled)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/set-the-logging-level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing Haystack\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmenting Training Data\n",
    "\n",
    "Having more training data is useful at all levels of model training. When performing intermediate layer distillation, additional data is beneficial, even if it is synthetically generated. Here we will be using the [`augment_squad.py` script](https://github.com/deepset-ai/haystack/blob/main/haystack/utils/augment_squad.py) to augment our dataset. It creates artifical copies of question answering samples by replacing randomly chosen words with words of similar meaning. This meaning similarity is determined by their vector representations in a GLoVe word embedding model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Download the `augment_squad.py` script."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/deepset-ai/haystack/main/haystack/utils/augment_squad.py"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Download a small slice of the SQuAD question answering database."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from haystack.utils import fetch_archive_from_http\n",
    "\n",
    "doc_dir = \"data/distil_a_reader\"\n",
    "squad_dir = doc_dir + \"/squad\"\n",
    "\n",
    "s3_url = \"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/squad_small.json.zip\"\n",
    "fetch_archive_from_http(url=s3_url, output_dir=squad_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " 3. Download a set of GLoVe vectors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "glove_dir = doc_dir + \"/glove\"\n",
    "\n",
    "glove_url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "fetch_archive_from_http(url=glove_url, output_dir=glove_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we have chosen a smaller set of vectors and a smaller dataset so that this tutorial will run in a reasonable amount of time. You will want to pick larger versions of both for real use cases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Run the `augment_squad.py` script to create an augmented dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python augment_squad.py \\\n",
    "    --squad_path data/distil_a_reader/squad/squad_small.json \\\n",
    "    --glove_path data/distil_a_reader/glove/glove.6B.300d.txt \\\n",
    "    --output_path augmented_dataset.json \\\n",
    "    --multiplication_factor 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The multiplication factor determines how many augmented samples we are generating. Setting it to 2 makes it much quicker to run. In real use cases, you will want to set this to something like 20."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distilling a Reader\n",
    "\n",
    "Distillation in Haystack is done in two distinct phases:\n",
    "- Intermediate layer distillation ensures that the teacher and student models behave similarly. This can be performed using the augmented data. While intermediate layer distillation is optional, it has positive impact on the result of model training.\n",
    "- Prediction layer distillation optimize the model for the specific task. This must be performed using the non-augmented data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Initialize the teacher model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "teacher = FARMReader(model_name_or_path=\"deepset/bert-base-uncased-squad2\", use_gpu=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are using [`deepset/bert-base-uncased-squad2`](https://huggingface.co/deepset/bert-base-uncased-squad2), a base sized BERT model trained on SQuAD."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Initialize the student model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "student = FARMReader(model_name_or_path=\"huawei-noah/TinyBERT_General_6L_768D\", use_gpu=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are using a TinyBERT model that is smaller than the teacher model. You can pick any other student model, so long as it uses the same tokenizer as the teacher model. Also, the number of layers in the teacher model must be a multiple of number of layers in the student."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Perform intermediate layer distillation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "student.distil_intermediate_layers_from(teacher, data_dir=\".\", train_filename=\"augmented_dataset.json\", use_gpu=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Perform prediction layer distillation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "student.distil_prediction_layer_from(teacher, data_dir=\"data/squad20\", train_filename=\"dev-v2.0.json\", use_gpu=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Save the student model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "student.save(directory=\"my_distilled_model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next Steps\n",
    "\n",
    "To learn how to measure the performance of these Reader models, see [Evaluate a Reader model](https://haystack.deepset.ai/tutorials/05_evaluate_a_reader)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
