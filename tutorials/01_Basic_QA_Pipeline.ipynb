{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your First Question Answering System\n",
    "\n",
    "- Level: Beginner\n",
    "- Time to complete: 20 minutes\n",
    "- Prerequisites: Prepare the Colab environment. See links below.\n",
    "- Nodes Used: `ElasticsearchDocumentStore`, `BM25Retriever`\n",
    "- Goal: After completing this tutorial, you will have built a question answering pipeline that can answer questions about the Game of Thrones series. The Python code in this notebook can be used on any operating system.\n",
    "\n",
    "This tutorial teaches you how to set up a question answering system that can search through complex knowledge bases, such as an internal wiki or a collection of financial reports. We will work on a set of Wikipedia pages about Game of Thrones. Let's learn how to build a question answering system and discover more about the marvellous seven kingdoms!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in GPU](https://docs.haystack.deepset.ai/v5.2-unstable/docs/enable-gpu-runtime-in-colab)\n",
    "- [Check if GPU is Enabled](https://docs.haystack.deepset.ai/v5.2-unstable/docs/check-if-gpu-is-enabled)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/v5.2-unstable/docs/set-the-logging-level)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Haystack\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DocumentStore\n",
    "\n",
    "A Haystack question answering system finds answers to questions within the documents stored in a `DocumentStore`. In this tutorial, we are initializing an `ElasticsearchDocumentStore` but there are many other options available. To learn which one is right for your use case, and how to initialize it, see [Choosing the Right Document Store](https://haystack.deepset.ai/components/document-store#choosing-the-right-document-store) and [Initialization](https://haystack.deepset.ai/components/document-store#initialisation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the Elasticsearch Image\n",
    "\n",
    "The `ElasticsearchDocumentStore` needs to attach to a running Elasticsearch server. Download, extract and set the permission for the Elasticsearch installation image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
    "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
    "chown -R daemon:daemon elasticsearch-7.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Start the Elasticsearch Server"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are working in an environment where Docker is available, you can also start Elasticsearch using Docker. You can do this [manually](https://haystack.deepset.ai/components/document-store#initialisation), or using our [`launch_es()`](https://haystack.deepset.ai/reference/utils) utility function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Create the DocumentStore\n",
    "\n",
    "When you initialize the `ElasticsearchDocumentStore` in Haystack, it opens a connection with the Elasticsearch service that we started in the previous step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "# Get the host where Elasticsearch is running, default to localhost\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "    host=host,\n",
    "    username=\"\",\n",
    "    password=\"\",\n",
    "    index=\"document\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Documents\n",
    "\n",
    "1. Download 517 articles from the Game of Thrones Wikipedia. You can find them in `data/tutorial1` as a set of `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from haystack.utils import fetch_archive_from_http\n",
    "\n",
    "doc_dir = \"data/tutorial1\"\n",
    "\n",
    "fetch_archive_from_http(\n",
    "    url=\"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt1.zip\",\n",
    "    output_dir=doc_dir\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Convert the files you just downloaded into Haystack [Document objects](https://haystack.deepset.ai/components/documents-answers-labels#document) to write them into the DocumentStore. Apply the `clean_wiki_text` cleaning function to the text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import clean_wiki_text, convert_files_to_docs\n",
    "docs = convert_files_to_docs(\n",
    "    dir_path=doc_dir,\n",
    "    clean_func=clean_wiki_text,\n",
    "    split_paragraphs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Write these Documents into the DocumentStore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now, let's write the dicts containing documents to our DB.\n",
    "document_store.write_documents(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While the default code in this tutorial uses Game of Thrones data, you can also supply your own. So long as your data adheres to the [input format](https://haystack.deepset.ai/components/document-store#input-format) or is cast into a [Document object](https://haystack.deepset.ai/components/documents-answers-labels#document), it can be written into the DocumentStore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Retriever\n",
    "\n",
    "Retrievers sift through all the Documents and return only those that it thinks might contain the answer to the question. Since this happens at [query time](), they need to be fast. Here we are using the BM25 algorithm which is considered a [sparse retrieval method](https://haystack.deepset.ai/pipeline_nodes/retriever#deeper-dive-dense-vs-sparse). For more Retriever options, see [Retriever](https://haystack.deepset.ai/pipeline_nodes/retriever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader\n",
    "\n",
    "A Reader scans the texts returned by Retrievers in detail and extracts the top answer candidates. Readers are based on powerful deep learning models but are much slower than Retrievers at processing the same amount of text. Haystack Readers can load question answering models from [Hugging Face's model hub](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads). Here we are using a base sized RoBERTa question answering model called [`deepset/roberta-base-squad2`](https://huggingface.co/deepset/roberta-base-squad2). To find out what model works best for your use case, see [Models](https://haystack.deepset.ai/pipeline_nodes/reader#models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Retriever-Reader Pipeline\n",
    "\n",
    "The Retriever and Reader that we just initialized are considered [nodes](https://haystack.deepset.ai/pipeline_nodes/overview) in Haystack and they are connected using a [`pipeline`](https://haystack.deepset.ai/components/pipelines). Pipelines are customizable, giving you the power to define how data is routed through the nodes at both indexing and querying time.\n",
    "\n",
    "It makes sense to join a Retriever and Reader because they are a complementary pairing. While the Reader is very effective at picking out answers to questions, it is not fast enough to perform this on large amounts of text at query time. By performing retrieval first, only the most promising candidate Documents are passed to the Reader, thus reducing its workload. This improvement in speed can come with a small tradeoff in accuracy. To learn how to optimize the performance of your question answering system, have a look at [Optimization](https://haystack.deepset.ai/guides/optimization).\n",
    "\n",
    "In Haystack, there is a `Pipeline` class that allows you to define your own pipeline configuration. However, there are also [Ready-Made Pipelines](https://haystack.deepset.ai/components/ready-made-pipelines) that simplify the initialization of commonly used configurations. Here, we are using the [`ExtractiveQAPipeline`](https://haystack.deepset.ai/components/ready-made-pipelines#extractiveqapipeline) that combines our Reader and Retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask a Question\n",
    "\n",
    "1. Use the pipeline `run()` method to ask a question. The query argument is where you type your question. Additionally, you can set the number of documents you want the Reader and Retriever to return using the `top-k` parameter. To learn more about setting arguments, see [Arguments](https://haystack.deepset.ai/components/pipelines#arguments). To understand the importance of the `top-k` parameter, see [Choosing the Right top-k Values](https://haystack.deepset.ai/guides/optimization#choosing-the-right-top-k-values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "prediction = pipe.run(\n",
    "    query=\"Who is the father of Arya Stark?\",\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 10},\n",
    "        \"Reader\": {\"top_k\": 5}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are some questions you could try out:\n",
    "- Who is the father of Arya Stark?\n",
    "- Who created the Dothraki vocabulary?\n",
    "- Who is the sister of Sansa?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The answers returned by the pipeline can be printed out directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Simplify the printed answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"minimum\" ## Choose from `minimum`, `medium` and `all`\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "And there you have it! Congratulations on building your first machine learning based question answering system!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
