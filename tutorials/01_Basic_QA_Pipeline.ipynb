{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your First Question Answering System\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deepset-ai/haystack-tutorials/blob/main/tutorials/01_Basic_QA_Pipeline.ipynb)\n",
    "\n",
    "Question answering allows you to quickly look into your Document collections to find answers to your questions. You can use it to search through complex knowledge bases, or long documents.\n",
    "\n",
    "A knowledge base could, for example, be your website, an internal wiki or a collection of financial reports. In this tutorial we will work on a set of wiki pages about Game of Thrones. Let's learn how to build a question answering system and discover more about the marvellous seven kingdoms!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Environment\n",
    "\n",
    "Before running the code in this notebook, you should set up the Colab environment with the following steps:\n",
    "- [Enable GPU Runtime in GPU]()\n",
    "- [Check if GPU is Enabled]()\n",
    "- [Set logging level to INFO]()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haystack Installation\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Store\n",
    "\n",
    "A Haystack question answering system finds answers to questions within the documents stored in a `DocumentStore`. In this tutorial, we are initializing an `ElasticsearchDocumentStore` but there are many other options available. To learn which one is right for your use case, and how to initialize it, see [Choosing the Right Document Store](https://haystack.deepset.ai/components/document-store#choosing-the-right-document-store) and [Initialization](https://haystack.deepset.ai/components/document-store#initialisation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start an Elasticsearch Server\n",
    "\n",
    "The `ElasticsearchDocumentStore` needs to attach to a running Elasticsearch server. To download, extract and set the permission for the Elasticsearch installation image, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
    "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
    "chown -R daemon:daemon elasticsearch-7.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start the server, run:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "\n",
    "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's wait 30 seconds to make sure the server has fully started up."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are working in an environment where Docker is available, you can also start Elasticsearch via Docker. This can be done [manually](https://haystack.deepset.ai/components/document-store#initialisation), or using our [`launch_es()`]() utility function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the DocumentStore\n",
    "\n",
    "When you initialize the `ElasticsearchDocumentStore`, it opens a connection with the Elasticsearch service."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "\n",
    "# Get the host where Elasticsearch is running, default to localhost\n",
    "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
    "\n",
    "document_store = ElasticsearchDocumentStore(\n",
    "    host=host,\n",
    "    username=\"\",\n",
    "    password=\"\",\n",
    "    index=\"document\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Documents\n",
    "\n",
    "Let's download 517 articles from the Game of Thrones Wikipedia. They can be found in `data/tutorial1` as a set of `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from haystack.utils import fetch_archive_from_http\n",
    "\n",
    "doc_dir = \"data/tutorial1\"\n",
    "\n",
    "fetch_archive_from_http(\n",
    "    url=\"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt1.zip\",\n",
    "    output_dir=doc_dir\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `.txt` files we just downloaded need to be converted into Haystack [Document objects](https://haystack.deepset.ai/components/documents-answers-labels#document) before they can be written into the DocumentStore. We will also apply the `clean_wiki_text` cleaning function to the text and split the Wikipedia documents by paragraph breaks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import clean_wiki_text, convert_files_to_docs\n",
    "docs = convert_files_to_docs(\n",
    "    dir_path=doc_dir,\n",
    "    clean_func=clean_wiki_text,\n",
    "    split_paragraphs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's write these Documents into the DocumentStore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now, let's write the dicts containing documents to our DB.\n",
    "document_store.write_documents(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While the default code in this tutorial uses Game of Thrones data, you can also supply your own data. So long as your data adheres to the [input format](https://haystack.deepset.ai/components/document-store#input-format) or is cast into a [Document object](https://haystack.deepset.ai/components/documents-answers-labels#document), it can be written into the DocumentStore."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever\n",
    "\n",
    "Retrievers sift through all the Documents and return only those that it thinks might contain the answer to the question. Since this happens at [query time](), they need to be fast. Here we are using the BM25 algorithm which is considered a [sparse retrieval method](https://haystack.deepset.ai/pipeline_nodes/retriever#deeper-dive-dense-vs-sparse). For more Retriever options, see [Retriever](https://haystack.deepset.ai/pipeline_nodes/retriever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader\n",
    "\n",
    "A Reader scans the texts returned by Retrievers in detail and extracts the top answer candidates. Readers are based on powerful deep learning models but are much slower than Retrievers at processing the same amount of text. Haystack Readers can load question answering models from [Hugging Face's model hub](https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads). Here we are using a base sized RoBERTa question answering model called [`deepset/roberta-base-squad2`](https://huggingface.co/deepset/roberta-base-squad2). To find out what model works best for your use case, see [Models](https://haystack.deepset.ai/pipeline_nodes/reader#models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Retriever-Reader Pipeline\n",
    "\n",
    "The Retriever and Reader that we just initialized are considered [nodes](https://haystack.deepset.ai/pipeline_nodes/overview) in Haystack and they are connected using a [`pipeline`](https://haystack.deepset.ai/components/pipelines). Pipelines are customizable, giving you the power to define how data is routed through the nodes at both indexing and querying time.\n",
    "\n",
    "It makes sense to join a Retriever and Reader because they are a complementary pairing. While the Reader is very effective at picking out answers to questions, it is not fast enough to perform this on large amounts of text at query time. By performing retrieval first, only the most promising candidate Documents are passed to the Reader, thus reducing its workload. This improvement in speed can come with a small tradeoff in accuracy. To learn how to optimize the performance of your question answering system, have a look at [Optimization](https://haystack.deepset.ai/guides/optimization).\n",
    "\n",
    "In Haystack, there is a `Pipeline` class that allows you to define your own pipeline configuration. However, there are also [Ready-Made Pipelines](https://haystack.deepset.ai/components/ready-made-pipelines) that simplify the initialization of commonly used configurations. Here, we are using the [`ExtractiveQAPipeline`](https://haystack.deepset.ai/components/ready-made-pipelines#extractiveqapipeline) that combines our Reader and Retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking Questions and Getting Answers\n",
    "\n",
    "Haystack pipelines have a `run()` method that performs a query, or in the case of an `ExtractiveQAPipeline`, answers a question. The `params` argument allows you to provide arguments to the nodes performing the query. See [Arguments](https://haystack.deepset.ai/components/pipelines#arguments) to learn how to populate this field, and [Choosing the Right top-k Values] to understand what the `top-k` parameters are doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "prediction = pipe.run(\n",
    "    query=\"Who is the father of Arya Stark?\",\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 10},\n",
    "        \"Reader\": {\"top_k\": 5}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are some questions you could try out:\n",
    "- Who is the father of Arya Stark?\n",
    "- Who created the Dothraki vocabulary?\n",
    "- Who is the sister of Sansa?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers returned by the pipeline can be printed out directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a utility functions to simplify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"minimum\" ## Choose from `minimum`, `medium` and `all`\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "And there you have it! Congratulations on building your first machine learning based question answering system!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
