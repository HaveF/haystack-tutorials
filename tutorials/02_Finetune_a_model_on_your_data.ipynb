{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fine-tuning a Model on Your Own Data\n",
    "\n",
    "- **Level**: Intermediate\n",
    "- **Time to complete**: 15 minutes\n",
    "- **Nodes Used**: `FARMReader`\n",
    "- **Goal**: After completing this tutorial, you will have learned hot to fine-tune a pretrained Reader model with your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "For many use cases it is sufficient to just use one of the existing public models that were trained on SQuAD or other public QA datasets (e.g. Natural Questions).\n",
    "However, if you have domain-specific questions, fine-tuning your model on custom examples will very likely boost your performance.\n",
    "While this varies by domain, we saw that ~ 2000 examples can easily increase performance by +5-20%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Preparing the Colab Environment\n",
    "\n",
    "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n",
    "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/log-level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Haystack\n",
    "\n",
    "To start, let's install the latest release of Haystack with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.0\n",
      "Requirement already satisfied: farm-haystack[colab] in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (1.12.2)\n",
      "Requirement already satisfied: python-docx in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (0.8.11)\n",
      "Requirement already satisfied: azure-ai-formrecognizer>=3.2.0b2 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (3.2.0)\n",
      "Requirement already satisfied: more-itertools in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (8.14.0)\n",
      "Requirement already satisfied: tika in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (1.24)\n",
      "Requirement already satisfied: elasticsearch<8,>=7.7 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (7.10.1)\n",
      "Requirement already satisfied: rank-bm25 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (0.2.2)\n",
      "Requirement already satisfied: pandas in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (1.5.0)\n",
      "Requirement already satisfied: requests in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (2.28.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (1.9.1)\n",
      "Requirement already satisfied: dill in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (0.3.6)\n",
      "Requirement already satisfied: posthog in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (2.1.2)\n",
      "Requirement already satisfied: jsonschema in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (4.4.0)\n",
      "Requirement already satisfied: langdetect in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (1.0.9)\n",
      "Requirement already satisfied: tqdm in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (4.64.1)\n",
      "Requirement already satisfied: nltk in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (3.7)\n",
      "Requirement already satisfied: networkx in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (2.8.6)\n",
      "Requirement already satisfied: rapidfuzz<2.8.0,>=2.0.15 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (2.7.0)\n",
      "Requirement already satisfied: mmh3 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (3.0.0)\n",
      "Requirement already satisfied: mlflow in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (1.0.0)\n",
      "Requirement already satisfied: quantulum3 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (0.7.10)\n",
      "Requirement already satisfied: transformers[torch]==4.25.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (4.25.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.5.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (0.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (1.1.2)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (2.2.2)\n",
      "Requirement already satisfied: seqeval in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (1.2.2)\n",
      "Requirement already satisfied: pydantic in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from farm-haystack[colab]) (1.10.2)\n",
      "Collecting pillow<=9.0.0\n",
      "  Downloading Pillow-9.0.0-cp310-cp310-macosx_10_10_universal2.whl (3.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (1.23.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (5.4.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.7 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (1.12.1)\n",
      "Requirement already satisfied: msrest>=0.6.21 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (0.7.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.25.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (4.3.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.1.28)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (1.26.12)\n",
      "Requirement already satisfied: certifi in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (2022.6.15)\n",
      "Requirement already satisfied: jarowinkler<2.0.0,>=1.2.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from rapidfuzz<2.8.0,>=2.0.15->farm-haystack[colab]) (1.2.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from sentence-transformers>=2.2.0->farm-haystack[colab]) (0.1.97)\n",
      "Requirement already satisfied: torchvision in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from sentence-transformers>=2.2.0->farm-haystack[colab]) (0.13.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from jsonschema->farm-haystack[colab]) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from jsonschema->farm-haystack[colab]) (21.4.0)\n",
      "Requirement already satisfied: six in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from langdetect->farm-haystack[colab]) (1.16.0)\n",
      "Requirement already satisfied: databricks-cli>=0.8.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (0.17.3)\n",
      "Requirement already satisfied: Flask in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (2.2.2)\n",
      "Requirement already satisfied: entrypoints in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (0.4)\n",
      "Requirement already satisfied: sqlparse in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (0.4.2)\n",
      "Requirement already satisfied: querystring-parser in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (1.2.4)\n",
      "Requirement already satisfied: cloudpickle in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (2.2.0)\n",
      "Requirement already satisfied: docker>=3.6.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (2.8.2)\n",
      "Requirement already satisfied: click>=7.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (8.1.3)\n",
      "Requirement already satisfied: simplejson in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (3.18.0)\n",
      "Requirement already satisfied: alembic in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (1.8.1)\n",
      "Requirement already satisfied: sqlalchemy in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (1.4.41)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (3.1.27)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (3.20.1)\n",
      "Requirement already satisfied: gunicorn in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from mlflow->farm-haystack[colab]) (20.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from requests->farm-haystack[colab]) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from requests->farm-haystack[colab]) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from pandas->farm-haystack[colab]) (2022.2.1)\n",
      "Requirement already satisfied: backoff<2.0.0,>=1.10.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from posthog->farm-haystack[colab]) (1.11.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from posthog->farm-haystack[colab]) (1.6)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from python-docx->farm-haystack[colab]) (4.9.1)\n",
      "Requirement already satisfied: inflect in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from quantulum3->farm-haystack[colab]) (6.0.0)\n",
      "Requirement already satisfied: num2words in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from quantulum3->farm-haystack[colab]) (0.5.12)\n",
      "Requirement already satisfied: setuptools in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from tika->farm-haystack[colab]) (65.6.3)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from databricks-cli>=0.8.0->mlflow->farm-haystack[colab]) (0.8.10)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from databricks-cli>=0.8.0->mlflow->farm-haystack[colab]) (3.2.1)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from databricks-cli>=0.8.0->mlflow->farm-haystack[colab]) (2.5.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from docker>=3.6.0->mlflow->farm-haystack[colab]) (1.4.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from gitpython>=2.1.0->mlflow->farm-haystack[colab]) (4.0.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.3.1)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (0.6.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]==4.25.1->farm-haystack[colab]) (3.0.9)\n",
      "Requirement already satisfied: Mako in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from alembic->mlflow->farm-haystack[colab]) (1.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from sqlalchemy->mlflow->farm-haystack[colab]) (1.1.3)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from Flask->mlflow->farm-haystack[colab]) (2.2.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from Flask->mlflow->farm-haystack[colab]) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from Flask->mlflow->farm-haystack[colab]) (2.1.2)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from num2words->quantulum3->farm-haystack[colab]) (0.6.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow->farm-haystack[colab]) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages (from Jinja2>=3.0->Flask->mlflow->farm-haystack[colab]) (2.1.1)\n",
      "Installing collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.2.0\n",
      "    Uninstalling Pillow-9.2.0:\n",
      "      Successfully uninstalled Pillow-9.2.0\n",
      "Successfully installed pillow-9.0.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Set the logging level to INFO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create Training Data\n",
    "\n",
    "There are two ways to generate training data\n",
    "\n",
    "1. **Annotation**: You can use the [annotation tool](https://haystack.deepset.ai/guides/annotation) to label your data, i.e. highlighting answers to your questions in a document. The tool supports structuring your workflow with organizations, projects, and users. The labels can be exported in SQuAD format that is compatible for training with Haystack.\n",
    "\n",
    "![Snapshot of the annotation tool](https://raw.githubusercontent.com/deepset-ai/haystack/main/docs/img/annotation_tool.png)\n",
    "\n",
    "2. **Feedback**: For production systems, you can collect training data from direct user feedback via Haystack's [REST API interface](https://github.com/deepset-ai/haystack#rest-api). This includes a customizable user feedback API for providing feedback on the answer returned by the API. The API provides a feedback export endpoint to obtain the feedback data for fine-tuning your model further.\n",
    "\n",
    "\n",
    "## Fine-tune your model\n",
    "\n",
    "Once you have collected training data, you can fine-tune your base model.\n",
    "We initialize a reader as a base model and fine-tune it on our own custom dataset (should be in SQuAD-like format).\n",
    "We recommend using a base model that was trained on SQuAD or a similar QA dataset before to benefit from Transfer Learning effects.\n",
    "\n",
    "**Recommendation**: Run training on a GPU.\n",
    "If you are using Colab: Enable this in the menu \"Runtime\" > \"Change Runtime type\" > Select \"GPU\" in dropdown.\n",
    "Then change the `use_gpu` arguments below to `True`\n",
    "\n",
    "1. Initialize a `Reader` with the model you would like to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuanacelik/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"distilbert-base-uncased-distilled-squad\", use_gpu=True, devices=[\"mps\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get SQuAD style data to be used for training. Below, you can fetch a dataset that we have already prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.utils import fetch_archive_from_http\n",
    "\n",
    "data_dir = \"data/fine-tuning\"\n",
    "\n",
    "\n",
    "fetch_archive_from_http(\n",
    "    url=\"https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/squad20.tar.gz\",\n",
    "    output_dir=data_dir\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model with yor won data and save it to \"my_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset: 100%|██████████| 3/3 [00:03<00:00,  1.30s/ Dicts]\n",
      "Train epoch 0/0 (Cur. train loss: 3.6723):   5%|▍         | 63/1360 [18:14<6:15:41, 17.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reader\u001b[39m.\u001b[39;49mtrain(data_dir\u001b[39m=\u001b[39;49mdata_dir, train_filename\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msquad20/dev-v2.0.json\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_gpu\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, n_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, save_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy_model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages/haystack/nodes/reader/farm.py:437\u001b[0m, in \u001b[0;36mFARMReader.train\u001b[0;34m(self, data_dir, train_filename, dev_filename, test_filename, use_gpu, devices, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes, use_amp, checkpoint_root_dir, checkpoint_every, checkpoints_to_keep, caching, cache_path, grad_acc_steps, early_stopping, max_query_length)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[1;32m    361\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    362\u001b[0m     data_dir: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     max_query_length: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    386\u001b[0m ):\n\u001b[1;32m    387\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39m    Fine-tune a model on a QA dataset. Options:\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39m    - Take a plain language model (e.g. `bert-base-cased`) and train it for QA (e.g. on SQuAD data)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39m    :return: None\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_training_procedure(\n\u001b[1;32m    438\u001b[0m         data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m    439\u001b[0m         train_filename\u001b[39m=\u001b[39;49mtrain_filename,\n\u001b[1;32m    440\u001b[0m         dev_filename\u001b[39m=\u001b[39;49mdev_filename,\n\u001b[1;32m    441\u001b[0m         test_filename\u001b[39m=\u001b[39;49mtest_filename,\n\u001b[1;32m    442\u001b[0m         use_gpu\u001b[39m=\u001b[39;49muse_gpu,\n\u001b[1;32m    443\u001b[0m         devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    444\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    445\u001b[0m         n_epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[1;32m    446\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    447\u001b[0m         max_seq_len\u001b[39m=\u001b[39;49mmax_seq_len,\n\u001b[1;32m    448\u001b[0m         warmup_proportion\u001b[39m=\u001b[39;49mwarmup_proportion,\n\u001b[1;32m    449\u001b[0m         dev_split\u001b[39m=\u001b[39;49mdev_split,\n\u001b[1;32m    450\u001b[0m         evaluate_every\u001b[39m=\u001b[39;49mevaluate_every,\n\u001b[1;32m    451\u001b[0m         save_dir\u001b[39m=\u001b[39;49msave_dir,\n\u001b[1;32m    452\u001b[0m         num_processes\u001b[39m=\u001b[39;49mnum_processes,\n\u001b[1;32m    453\u001b[0m         use_amp\u001b[39m=\u001b[39;49muse_amp,\n\u001b[1;32m    454\u001b[0m         checkpoint_root_dir\u001b[39m=\u001b[39;49mcheckpoint_root_dir,\n\u001b[1;32m    455\u001b[0m         checkpoint_every\u001b[39m=\u001b[39;49mcheckpoint_every,\n\u001b[1;32m    456\u001b[0m         checkpoints_to_keep\u001b[39m=\u001b[39;49mcheckpoints_to_keep,\n\u001b[1;32m    457\u001b[0m         caching\u001b[39m=\u001b[39;49mcaching,\n\u001b[1;32m    458\u001b[0m         cache_path\u001b[39m=\u001b[39;49mcache_path,\n\u001b[1;32m    459\u001b[0m         grad_acc_steps\u001b[39m=\u001b[39;49mgrad_acc_steps,\n\u001b[1;32m    460\u001b[0m         early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m    461\u001b[0m         max_query_length\u001b[39m=\u001b[39;49mmax_query_length,\n\u001b[1;32m    462\u001b[0m         distributed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages/haystack/nodes/reader/farm.py:357\u001b[0m, in \u001b[0;36mFARMReader._training_procedure\u001b[0;34m(self, data_dir, train_filename, dev_filename, test_filename, use_gpu, devices, batch_size, n_epochs, learning_rate, max_seq_len, warmup_proportion, dev_split, evaluate_every, save_dir, num_processes, use_amp, checkpoint_root_dir, checkpoint_every, checkpoints_to_keep, teacher_model, teacher_batch_size, caching, cache_path, distillation_loss_weight, distillation_loss, temperature, tinybert, processor, grad_acc_steps, early_stopping, distributed, doc_stride, max_query_length)\u001b[0m\n\u001b[1;32m    338\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer\u001b[39m.\u001b[39mcreate_or_load_checkpoint(\n\u001b[1;32m    339\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    340\u001b[0m         optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[39m# 5. Let it grow!\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minferencer\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    358\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(Path(save_dir))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages/haystack/modeling/training/base.py:217\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39m# Move batch of samples to device\u001b[39;00m\n\u001b[1;32m    216\u001b[0m batch \u001b[39m=\u001b[39m {key: batch[key]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m batch}\n\u001b[0;32m--> 217\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(batch, step)\n\u001b[1;32m    219\u001b[0m \u001b[39m# Perform  evaluation\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_every \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    222\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    223\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    224\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_rank \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    225\u001b[0m ):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages/haystack/modeling/training/base.py:322\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, batch, step)\u001b[0m\n\u001b[1;32m    320\u001b[0m     per_sample_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mlogits_to_loss(logits\u001b[39m=\u001b[39mlogits, global_step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m    321\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madjust_loss(per_sample_loss)\n\u001b[0;32m--> 322\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward_propagate(loss, step)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages/haystack/modeling/training/base.py:331\u001b[0m, in \u001b[0;36mTrainer.backward_propagate\u001b[0;34m(self, loss, step)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_learning_rate:\n\u001b[1;32m    329\u001b[0m             tracker\u001b[39m.\u001b[39mtrack_metrics({\u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_schedule\u001b[39m.\u001b[39mget_last_lr()[\u001b[39m0\u001b[39m]}, step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mglobal_step)\n\u001b[0;32m--> 331\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_acc_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/haystack-tutorials/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "reader.train(data_dir=data_dir, train_filename=\"squad20/dev-v2.0.json\", use_gpu=True, n_epochs=1, save_dir=\"my_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "4. Now initialize a new reader with your fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_reader = FARMReader(model_name_or_path=\"my_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finaly, try using the `new_reader` that was initialized with your fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.schema import Document\n",
    "\n",
    "new_reader.predict(query=\"What is the capital of Germany?\", documents=[Document(content=\"The capital of Germany is Berlin\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## About us\n",
    "\n",
    "This [Haystack](https://github.com/deepset-ai/haystack/) notebook was made with love by [deepset](https://deepset.ai/) in Berlin, Germany\n",
    "\n",
    "We bring NLP to the industry via open source!  \n",
    "Our focus: Industry specific language models & large scale QA systems.  \n",
    "  \n",
    "Some of our other work: \n",
    "- [German BERT](https://deepset.ai/german-bert)\n",
    "- [GermanQuAD and GermanDPR](https://deepset.ai/germanquad)\n",
    "- [FARM](https://github.com/deepset-ai/FARM)\n",
    "\n",
    "Get in touch:\n",
    "[Twitter](https://twitter.com/deepset_ai) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community/join) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://deepset.ai)\n",
    "\n",
    "By the way: [we're hiring!](https://www.deepset.ai/jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('haystack-tutorials')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e4fa2e1c496b8379da88afac82c60055e1be33cd79040f849449f398c153e43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
